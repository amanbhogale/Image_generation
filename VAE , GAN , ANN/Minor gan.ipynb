{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc73fd53-948a-48a7-ac3f-ab18b4d92950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 10:53:46.245538: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-06 10:53:46.779416: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-06 10:53:46.782145: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 10:53:47.866178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import BatchNormalization, Input, Dense, Reshape,Flatten\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from keras.layers import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cd04e1-5afd-4273-95e5-7fadde42eab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_dim: int):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=latent_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(np.prod((28, 28, 1)), activation='tanh'),\n",
    "        # reshape to MNIST image size\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "    model.summary()\n",
    "    # the latent input vector z\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    generated = model(z)\n",
    "    # build model from the input and output\n",
    "    return Model(z, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451ee75b-0bd4-4d8e-af6d-8500299c3744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dense(128),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "    ], name='discriminator')\n",
    "    model.summary()\n",
    "    image = Input(shape=(28, 28, 1))\n",
    "    output = model(image)\n",
    "    return Model(image, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487292e4-7710-46ec-bb9e-ea574b58b8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(generator, discriminator, combined, steps, batch_size):\n",
    "\n",
    "    # Load the dataset\n",
    "    (x_train, _), _ = mnist.load_data()\n",
    "    # Rescale in [-1, 1] interval\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5 \n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "# Discriminator ground truths\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    latent_dim = generator.input_shape[1]\n",
    "    for step in range(steps):\n",
    "    # Train the discriminator\n",
    "    # Select a random batch of images\n",
    "        real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
    "        # Random batch of noise\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        # Generate a batch of new images\n",
    "        generated_images = generator.predict(noise)\n",
    "        # Train the discriminator\n",
    "        discriminator_real_loss = discriminator.train_on_batch(real_images, real)\n",
    "        discriminator_fake_loss = discriminator.train_on_batch(generated_images, fake)\n",
    "        discriminator_loss = 0.5 * np.add(discriminator_real_loss, discriminator_fake_loss)\n",
    "\n",
    "        # Train the generator\n",
    "        # random latent vector z\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        # Train the generator\n",
    "        # Note that we use the \"valid\" labels for the generated images\n",
    "        # That's because we try to maximize the discriminator loss\n",
    "        generator_loss = combined.train_on_batch(noise, real)\n",
    "\n",
    "        # Display progress\n",
    "        print(\"%d [Discriminator loss: %.4f%%, acc.: %.2f%%] [Generator loss: %.4f%%]\" %\n",
    "              (step, discriminator_loss[0], 100 * discriminator_loss[1], generator_loss))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8835137e-238e-493b-8e73-aed768c96a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "def plot_generated_images(generator):\n",
    "    \"\"\"\n",
    "    Display a nxn 2D manifold of digits\n",
    "    :param generator: the generator\n",
    "    \"\"\"\n",
    "    n = 10\n",
    "    digit_size = 28\n",
    "    # big array containing all images\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    latent_dim = generator.input_shape[1]\n",
    "    # n*n random latent distributions\n",
    "    noise = np.random.normal(0, 1, (n * n, latent_dim))\n",
    "    # generate the images\n",
    "    generated_images = generator.predict(noise)\n",
    "    # fill the big array with images\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            slice_i = slice(i * digit_size, (i + 1) * digit_size)\n",
    "            slice_j = slice(j * digit_size, (j + 1) * digit_size)\n",
    "            figure[slice_i, slice_j] = np.reshape(generated_images[i * n + j], (28, 28))\n",
    "    # plot the results\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a802fc-5c72-4e0f-aa39-0ecb0416a806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 233985 (914.00 KB)\n",
      "Trainable params: 233985 (914.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 10:53:55.156355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-06 10:53:55.335736: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 784)               402192    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 578704 (2.21 MB)\n",
      "Trainable params: 576912 (2.20 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "0 [Discriminator loss: 0.8632%, acc.: 32.03%] [Generator loss: 0.9707%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1 [Discriminator loss: 0.3235%, acc.: 90.62%] [Generator loss: 0.8314%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2 [Discriminator loss: 0.3342%, acc.: 81.25%] [Generator loss: 0.7592%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "3 [Discriminator loss: 0.3356%, acc.: 78.12%] [Generator loss: 0.8030%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4 [Discriminator loss: 0.3411%, acc.: 77.34%] [Generator loss: 0.8604%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "5 [Discriminator loss: 0.3221%, acc.: 83.98%] [Generator loss: 1.0472%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "6 [Discriminator loss: 0.2717%, acc.: 93.75%] [Generator loss: 1.2217%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "7 [Discriminator loss: 0.2008%, acc.: 98.44%] [Generator loss: 1.4964%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "8 [Discriminator loss: 0.1541%, acc.: 99.61%] [Generator loss: 1.7257%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "9 [Discriminator loss: 0.1287%, acc.: 100.00%] [Generator loss: 1.9735%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "10 [Discriminator loss: 0.1138%, acc.: 100.00%] [Generator loss: 2.2167%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "11 [Discriminator loss: 0.0864%, acc.: 100.00%] [Generator loss: 2.4015%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "12 [Discriminator loss: 0.0808%, acc.: 100.00%] [Generator loss: 2.5793%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "13 [Discriminator loss: 0.0677%, acc.: 100.00%] [Generator loss: 2.6612%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "14 [Discriminator loss: 0.0702%, acc.: 100.00%] [Generator loss: 2.8062%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "15 [Discriminator loss: 0.0610%, acc.: 100.00%] [Generator loss: 2.9074%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "16 [Discriminator loss: 0.0564%, acc.: 100.00%] [Generator loss: 2.9940%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "17 [Discriminator loss: 0.0531%, acc.: 100.00%] [Generator loss: 3.0448%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "18 [Discriminator loss: 0.0481%, acc.: 100.00%] [Generator loss: 3.1168%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "19 [Discriminator loss: 0.0503%, acc.: 100.00%] [Generator loss: 3.1666%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "20 [Discriminator loss: 0.0418%, acc.: 100.00%] [Generator loss: 3.2167%]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "21 [Discriminator loss: 0.0432%, acc.: 100.00%] [Generator loss: 3.2471%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "22 [Discriminator loss: 0.0421%, acc.: 100.00%] [Generator loss: 3.3618%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "23 [Discriminator loss: 0.0346%, acc.: 100.00%] [Generator loss: 3.3528%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "24 [Discriminator loss: 0.0370%, acc.: 100.00%] [Generator loss: 3.3022%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "25 [Discriminator loss: 0.0347%, acc.: 100.00%] [Generator loss: 3.3373%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "26 [Discriminator loss: 0.0314%, acc.: 100.00%] [Generator loss: 3.3908%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "27 [Discriminator loss: 0.0382%, acc.: 100.00%] [Generator loss: 3.5110%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "28 [Discriminator loss: 0.0328%, acc.: 100.00%] [Generator loss: 3.4966%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "29 [Discriminator loss: 0.0310%, acc.: 100.00%] [Generator loss: 3.5769%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "30 [Discriminator loss: 0.0353%, acc.: 100.00%] [Generator loss: 3.6869%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "31 [Discriminator loss: 0.0338%, acc.: 100.00%] [Generator loss: 3.7922%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "32 [Discriminator loss: 0.0242%, acc.: 100.00%] [Generator loss: 3.8014%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "33 [Discriminator loss: 0.0281%, acc.: 100.00%] [Generator loss: 3.7709%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "34 [Discriminator loss: 0.0331%, acc.: 100.00%] [Generator loss: 3.7737%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "35 [Discriminator loss: 0.0325%, acc.: 100.00%] [Generator loss: 4.0665%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "36 [Discriminator loss: 0.0336%, acc.: 100.00%] [Generator loss: 3.9704%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "37 [Discriminator loss: 0.0471%, acc.: 99.22%] [Generator loss: 4.3415%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "38 [Discriminator loss: 0.1727%, acc.: 91.80%] [Generator loss: 3.9857%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "39 [Discriminator loss: 0.0895%, acc.: 96.88%] [Generator loss: 4.7078%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "40 [Discriminator loss: 0.0200%, acc.: 100.00%] [Generator loss: 5.0357%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "41 [Discriminator loss: 0.0474%, acc.: 99.61%] [Generator loss: 4.6282%]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "42 [Discriminator loss: 0.0458%, acc.: 99.61%] [Generator loss: 4.1840%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "43 [Discriminator loss: 0.0577%, acc.: 99.61%] [Generator loss: 4.8142%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "44 [Discriminator loss: 1.1500%, acc.: 63.67%] [Generator loss: 3.6364%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "45 [Discriminator loss: 1.0966%, acc.: 72.66%] [Generator loss: 3.8973%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "46 [Discriminator loss: 0.4592%, acc.: 80.08%] [Generator loss: 3.3294%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "47 [Discriminator loss: 0.2341%, acc.: 87.11%] [Generator loss: 3.3779%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "48 [Discriminator loss: 0.1280%, acc.: 95.31%] [Generator loss: 3.2922%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "49 [Discriminator loss: 0.1156%, acc.: 96.48%] [Generator loss: 3.2387%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "50 [Discriminator loss: 0.1083%, acc.: 96.48%] [Generator loss: 3.1667%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "51 [Discriminator loss: 0.1115%, acc.: 98.83%] [Generator loss: 2.8726%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "52 [Discriminator loss: 0.1500%, acc.: 98.05%] [Generator loss: 2.7844%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "53 [Discriminator loss: 0.1579%, acc.: 98.05%] [Generator loss: 2.6819%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "54 [Discriminator loss: 0.1462%, acc.: 97.27%] [Generator loss: 2.6496%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "55 [Discriminator loss: 0.2204%, acc.: 94.14%] [Generator loss: 2.4892%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "56 [Discriminator loss: 0.1397%, acc.: 97.66%] [Generator loss: 3.0565%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "57 [Discriminator loss: 0.4218%, acc.: 75.39%] [Generator loss: 2.0110%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "58 [Discriminator loss: 0.2026%, acc.: 90.23%] [Generator loss: 3.2223%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "59 [Discriminator loss: 0.0843%, acc.: 99.61%] [Generator loss: 3.8041%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "60 [Discriminator loss: 0.8590%, acc.: 55.86%] [Generator loss: 0.9098%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "61 [Discriminator loss: 0.4918%, acc.: 67.19%] [Generator loss: 2.2580%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "62 [Discriminator loss: 0.1274%, acc.: 96.48%] [Generator loss: 3.6926%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "63 [Discriminator loss: 0.0905%, acc.: 100.00%] [Generator loss: 3.2285%]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "64 [Discriminator loss: 0.2395%, acc.: 94.14%] [Generator loss: 2.1464%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     28\u001b[0m combined\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     29\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0002\u001b[39m, beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# train the GAN system\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# display some random generated images\u001b[39;00m\n\u001b[1;32m     40\u001b[0m plot_generated_images(generator)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, combined, steps, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, latent_dim))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Generate a batch of new images\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the discriminator\u001b[39;00m\n\u001b[1;32m     21\u001b[0m discriminator_real_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_images, real)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/keras/src/engine/training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2513\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2518\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2519\u001b[0m         )\n\u001b[0;32m-> 2521\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1284\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:396\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:163\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    161\u001b[0m     num_parallel_calls, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_parallel_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m--> 163\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_map_dataset_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:5796\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   5794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   5795\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5796\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5797\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParallelMapDatasetV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5798\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5799\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_inter_op_parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5800\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeterministic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreserve_cardinality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   5803\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    latent_dim = 64\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=0.0002, beta_1=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Build the generator\n",
    "    generator = build_generator(latent_dim)\n",
    "\n",
    "    # Generator input z\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    generated_image = generator(z)\n",
    "\n",
    "    # Only train the generator for the combined model\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # The discriminator takes generated image as input and determines validity\n",
    "    real_or_fake = discriminator(generated_image)\n",
    "\n",
    "    # Stack the generator and discriminator in a combined model\n",
    "    # Trains the generator to deceive the discriminator\n",
    "    combined = Model(z, real_or_fake)\n",
    "    combined.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=0.0002, beta_1=0.5)\n",
    "    )\n",
    "\n",
    "    # train the GAN system\n",
    "    train(generator=generator,\n",
    "          discriminator=discriminator,\n",
    "          combined=combined,\n",
    "          steps=15000,\n",
    "          batch_size=128)\n",
    "    # display some random generated images\n",
    "    plot_generated_images(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ab27e-3b75-4230-8a97-2d7feac2c28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d2968-ea28-4184-9e10-9fdea6251573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e710484-2acd-4239-befb-cc28322fc1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
