{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c7cf78-5dd0-45f2-8889-29b8ba79af22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import MarkerStyle\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Lambda , Input ,Dense\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.models import Model \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43358973-4200-4e39-b65f-86e505882010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3860d81c-1cba-4250-b030-016d605eb293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "img_dim = (28, 28)  # Image dimension for MNIST dataset\n",
    "\n",
    "init = glorot_uniform(seed=42)\n",
    "# Assuming you have defined `latent_dim`, `init`, and `img_dim` elsewhere\n",
    "latent_dim = 100  # You can choose an appropriate dimension for your latent space\n",
    "\n",
    "# Create a Sequential model for the generator\n",
    "generator = Sequential()\n",
    "\n",
    "# Input layer and hidden layer 1\n",
    "generator.add(Dense(128, input_shape=(latent_dim,), kernel_initializer=init))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "# Hidden layer 2\n",
    "generator.add(Dense(256))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "# Hidden layer 3\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "# Output layer\n",
    "generator.add(Dense(np.prod(img_dim), activation='tanh'))\n",
    "generator.add(Reshape(img_dim))\n",
    "\n",
    "# The generator model is now complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65518652-d2b0-4a64-b4c1-fb73a80bc573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LeakyReLU, Flatten, Input\n",
    "\n",
    "# Define the image dimensions for MNIST dataset\n",
    "img_dim = (28, 28)\n",
    "\n",
    "# Define the weight initializer (e.g., Glorot/Xavier initializer)\n",
    "from keras.initializers import glorot_uniform\n",
    "init = glorot_uniform(seed=42)\n",
    "\n",
    "# Create a Sequential model for the discriminator\n",
    "discriminator = Sequential()\n",
    "\n",
    "# Input layer (Flatten the image to a vector)\n",
    "discriminator.add(Flatten(input_shape=(img_dim[0], img_dim[1])))\n",
    "\n",
    "# Hidden layer 1\n",
    "discriminator.add(Dense(128, kernel_initializer=init))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 2\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Output layer\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# The discriminator model is now complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fb49129-2721-4c9c-acee-87403471e36b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_25 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_20 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 784)               402192    \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 28, 28)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 583312 (2.23 MB)\n",
      "Trainable params: 583312 (2.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 265601 (1.01 MB)\n",
      "Trainable params: 265601 (1.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_17 (Sequential)  (None, 28, 28)            583312    \n",
      "                                                                 \n",
      " sequential_18 (Sequential)  (None, 1)                 265601    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 848913 (3.24 MB)\n",
      "Trainable params: 583312 (2.23 MB)\n",
      "Non-trainable params: 265601 (1.01 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Define the image dimensions for MNIST dataset\n",
    "img_dim = (28, 28)\n",
    "\n",
    "# Define the weight initializer (e.g., Glorot/Xavier initializer)\n",
    "from keras.initializers import glorot_uniform\n",
    "init = glorot_uniform(seed=42)\n",
    "\n",
    "# Update the generator model to accept the correct input shape\n",
    "generator = Sequential()\n",
    "\n",
    "# Input layer and hidden layer 1\n",
    "generator.add(Dense(128, input_shape=(latent_dim,), kernel_initializer=init))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 2\n",
    "generator.add(Dense(256))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Output layer\n",
    "generator.add(Dense(np.prod(img_dim), activation='tanh'))\n",
    "generator.add(Reshape(img_dim))\n",
    "\n",
    "\n",
    "# The generator model summary\n",
    "generator.summary()\n",
    "\n",
    "# Create a Sequential model for the discriminator\n",
    "discriminator = Sequential()\n",
    "\n",
    "# Input layer (Flatten the image to a vector)\n",
    "discriminator.add(Flatten(input_shape=(img_dim[0], img_dim[1])))\n",
    "\n",
    "# Hidden layer 1\n",
    "discriminator.add(Dense(128, kernel_initializer=init))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 2\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Output layer\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# The discriminator model summary\n",
    "discriminator.summary()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Create a Sequential model for the GAN\n",
    "d_g = Sequential()\n",
    "d_g.add(generator)\n",
    "d_g.add(discriminator)\n",
    "\n",
    "# Compile the GAN model\n",
    "d_g.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "# The GAN model summary\n",
    "d_g.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2f7eaf5-77d2-4038-ae4a-4f57b89fbdf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected tuning parameters:\n",
      "Epochs: 150\n",
      "Number of batches: 15\n",
      "Batch size: 8\n",
      "Latent dimension: 128\n",
      "Smooth factor: 0.95\n",
      "Save interval: 20\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(batch_size, latent_dim))\n\u001b[0;32m     30\u001b[0m X_fake \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict_on_batch(z)\n\u001b[1;32m---> 31\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(x\u001b[38;5;241m=\u001b[39mX_fake, y\u001b[38;5;241m=\u001b[39mfake)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Discriminator loss\u001b[39;00m\n\u001b[0;32m     34\u001b[0m d_loss_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (d_loss_real[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m d_loss_fake[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fake' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Randomly set tuning parameters\n",
    "epochs = 150\n",
    "num_batches = 15\n",
    "batch_size = 8\n",
    "latent_dim = 128\n",
    "smooth = 0.95\n",
    "save_interval = 20  # Save generated images every 20 epochs\n",
    "\n",
    "# Print the randomly selected tuning parameters\n",
    "print(f\"Randomly selected tuning parameters:\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Smooth factor: {smooth}\")\n",
    "print(f\"Save interval: {save_interval}\")\n",
    "for epoch in range(epochs):  # Define and loop over epochs\n",
    "    for i in range(num_batches):\n",
    "        # Train Discriminator weights\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Real samples\n",
    "        X_batch = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "        d_loss_real = discriminator.train_on_batch(x=X_batch, y=real * (1 - smooth))\n",
    "        \n",
    "        # Fake Samples\n",
    "        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
    "        X_fake = generator.predict_on_batch(z)\n",
    "        d_loss_fake = discriminator.train_on_batch(x=X_fake, y=fake)\n",
    "        \n",
    "        # Discriminator loss\n",
    "        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
    "        \n",
    "        # Train Generator weights\n",
    "        discriminator.trainable = False\n",
    "        d_g_loss_batch = d_g.train_on_batch(x=z, y=real)\n",
    "        \n",
    "        # Optionally, print or log the loss values for monitoring\n",
    "        print(f\"Epoch: {epoch}, Batch: {i}, D Loss: {d_loss_batch}, G Loss: {d_g_loss_batch}\")\n",
    "\n",
    "    # Optionally, you can generate and save some sample images at specific intervals\n",
    "    if epoch % save_interval == 0:\n",
    "        generated_images = generator.predict(random_latent_vectors)\n",
    "        save_generated_images(generated_images, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5533b127-f0d5-4a30-a738-81baa4a2d5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1130, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_25/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m train_discriminator()\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Train the generator\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m train_generator()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Append the loss values to the history lists\u001b[39;00m\n\u001b[0;32m     49\u001b[0m d_loss_history\u001b[38;5;241m.\u001b[39mappend(d_loss[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[30], line 36\u001b[0m, in \u001b[0;36mtrain_generator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(batch_size, latent_dim)  \u001b[38;5;66;03m# Use noise vectors as input\u001b[39;00m\n\u001b[0;32m     35\u001b[0m valid_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 36\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m d_g\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, valid_labels)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:2763\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2759\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2761\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2763\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   2765\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filelwehhapm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1360\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1357\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m     )\n\u001b[0;32m   1359\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(run_step, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[0;32m   1361\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1362\u001b[0m     outputs,\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1364\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1365\u001b[0m )\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1349\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(data)\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1130\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:544\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m  None\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    543\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39minterim\u001b[38;5;241m.\u001b[39mmaybe_merge_call(\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_apply_gradients_fn,\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy,\n\u001b[0;32m   1256\u001b[0m     grads_and_vars,\n\u001b[0;32m   1257\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[38;5;241m=\u001b[39m(grad,), group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:233\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate different parts of the model separately. Please call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimizer.build(variables)` with the full list of trainable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables before the training loop or use legacy optimizer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(gradient, variable)\n",
      "\u001b[1;31mKeyError\u001b[0m: in user code:\n\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1130, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_25/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of epochs and other training parameters\n",
    "epochs = 100\n",
    "num_batches = 10\n",
    "batch_size = 5\n",
    "latent_dim = 128\n",
    "smooth = 0.95\n",
    "save_interval = 20  # Save generated images every 20 epochs\n",
    "real = np.ones((batch_size, 1))\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "d_loss_history = []\n",
    "d_g_loss_history = []\n",
    "\n",
    "# Define placeholder functions for training the discriminator and generator\n",
    "def train_discriminator():\n",
    "    # Generate real and fake data\n",
    "    real_data = np.random.rand(batch_size, *img_dim)  # Replace with real data\n",
    "    fake_data = generator.predict(np.random.rand(batch_size, latent_dim))\n",
    "\n",
    "    # Compute discriminator loss\n",
    "    real_labels = np.ones((batch_size, 1)) * smooth\n",
    "    fake_labels = np.zeros((batch_size, 1))\n",
    "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    return d_loss\n",
    "\n",
    "def train_generator():\n",
    "    # Generate fake data and calculate the generator loss\n",
    "    fake_data = np.random.rand(batch_size, latent_dim)  # Use noise vectors as input\n",
    "    valid_labels = np.ones((batch_size, 1))\n",
    "    g_loss = d_g.train_on_batch(fake_data, valid_labels)\n",
    "    return g_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_batches):\n",
    "        # Train the discriminator\n",
    "        d_loss = train_discriminator()\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = train_generator()\n",
    "\n",
    "    # Append the loss values to the history lists\n",
    "    d_loss_history.append(d_loss[0])\n",
    "    d_g_loss_history.append(g_loss[0])\n",
    "\n",
    "    # Optionally, print or log the loss values for monitoring\n",
    "    print(f\"Epoch: {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss[0]}\")\n",
    "\n",
    "    # Optionally, you can generate and save some sample images at specific intervals\n",
    "    if epoch % save_interval == 0:\n",
    "        generated_images = generator.predict(np.random.rand(10, latent_dim))\n",
    "        save_generated_images(generated_images, epoch)\n",
    "\n",
    "# Plot the discriminator and generator loss metrics\n",
    "plt.plot(d_loss_history, label='Discriminator Loss')\n",
    "plt.plot(d_g_loss_history, label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Discriminator and Generator Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3ed5cd4-5514-45f8-b188-e0c9bb32f41e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_41 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_32 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 784)               402192    \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 28, 28)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 583312 (2.23 MB)\n",
      "Trainable params: 583312 (2.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027CCCF64CC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027CCCF64CC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1130, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_25/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m train_discriminator()\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Train the generator\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m train_generator()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Append the loss values to the history lists\u001b[39;00m\n\u001b[0;32m     84\u001b[0m d_loss_history\u001b[38;5;241m.\u001b[39mappend(d_loss[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[33], line 69\u001b[0m, in \u001b[0;36mtrain_generator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[0;32m     68\u001b[0m valid_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 69\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m d_g\u001b[38;5;241m.\u001b[39mtrain_on_batch(noise, valid_labels)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:2763\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2759\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2761\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2763\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   2765\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filelwehhapm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1360\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1357\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m     )\n\u001b[0;32m   1359\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(run_step, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[0;32m   1361\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1362\u001b[0m     outputs,\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1364\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1365\u001b[0m )\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1349\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(data)\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1130\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:544\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m  None\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    543\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39minterim\u001b[38;5;241m.\u001b[39mmaybe_merge_call(\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_apply_gradients_fn,\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy,\n\u001b[0;32m   1256\u001b[0m     grads_and_vars,\n\u001b[0;32m   1257\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[38;5;241m=\u001b[39m(grad,), group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:233\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate different parts of the model separately. Please call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimizer.build(variables)` with the full list of trainable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables before the training loop or use legacy optimizer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(gradient, variable)\n",
      "\u001b[1;31mKeyError\u001b[0m: in user code:\n\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1130, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_25/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Define the image dimensions for MNIST dataset\n",
    "img_dim = (28, 28)\n",
    "\n",
    "# Define the weight initializer (e.g., Glorot/Xavier initializer)\n",
    "from keras.initializers import glorot_uniform\n",
    "init = glorot_uniform(seed=42)\n",
    "\n",
    "# Create a Sequential model for the generator\n",
    "generator = Sequential()\n",
    "\n",
    "# Input layer and hidden layer 1\n",
    "generator.add(Dense(128, input_shape=(latent_dim,), kernel_initializer=init))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 2\n",
    "generator.add(Dense(256))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Output layer\n",
    "generator.add(Dense(np.prod(img_dim), activation='tanh'))\n",
    "generator.add(Reshape(img_dim))\n",
    "\n",
    "# The generator model summary\n",
    "generator.summary()\n",
    "\n",
    "# Define the number of epochs and other training parameters\n",
    "epochs = 100\n",
    "num_batches = 10\n",
    "batch_size = 5\n",
    "latent_dim = 128\n",
    "smooth = 0.95\n",
    "save_interval = 20  # Save generated images every 20 epochs\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "d_loss_history = []\n",
    "d_g_loss_history = []\n",
    "\n",
    "# Define placeholder functions for training the discriminator and generator\n",
    "def train_discriminator():\n",
    "    # Generate real and fake data\n",
    "    real_data = np.random.rand(batch_size, *img_dim)  # Replace with real data\n",
    "    fake_data = generator.predict(np.random.rand(batch_size, latent_dim))\n",
    "\n",
    "    # Compute discriminator loss\n",
    "    real_labels = np.ones((batch_size, 1)) * smooth\n",
    "    fake_labels = np.zeros((batch_size, 1))\n",
    "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    return d_loss\n",
    "\n",
    "def train_generator():\n",
    "    # Generate fake data and calculate the generator loss\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    generated_images = generator.predict(noise)\n",
    "    valid_labels = np.ones((batch_size, 1))\n",
    "    g_loss = d_g.train_on_batch(noise, valid_labels)\n",
    "    return g_loss\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_batches):\n",
    "        # Train the discriminator\n",
    "        d_loss = train_discriminator()\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = train_generator()\n",
    "\n",
    "    # Append the loss values to the history lists\n",
    "    d_loss_history.append(d_loss[0])\n",
    "    d_g_loss_history.append(g_loss[0])\n",
    "\n",
    "    # Optionally, print or log the loss values for monitoring\n",
    "    print(f\"Epoch: {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss[0]}\")\n",
    "\n",
    "    # Optionally, you can generate and save some sample images at specific intervals\n",
    "    if epoch % save_interval == 0:\n",
    "        generated_images = generator.predict(np.random.randn(10, latent_dim))\n",
    "        save_generated_images(generated_images, epoch)\n",
    "\n",
    "# Plot the discriminator and generator loss metrics\n",
    "plt.plot(d_loss_history, label='Discriminator Loss')\n",
    "plt.plot(d_g_loss_history, label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Discriminator and Generator Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bcb5cd0-1725-41cf-9cc7-5b5029ab5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027CCD2BA660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027CCD2BA660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_23\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(64, 784)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, latent_dim))\n\u001b[0;32m     60\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[1;32m---> 61\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_images, labels_real)\n\u001b[0;32m     62\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_images, labels_fake)\n\u001b[0;32m     63\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:2763\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2759\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2761\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2763\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   2765\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filelwehhapm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1360\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1357\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m     )\n\u001b[0;32m   1359\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(run_step, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[0;32m   1361\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1362\u001b[0m     outputs,\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1364\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1365\u001b[0m )\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1349\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(data)\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:1126\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# Run forward pass.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m-> 1126\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1127\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\asus\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_23\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(64, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, LeakyReLU, Flatten, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, _), (_, _) = mnist.load_data()\n",
    "train_images = train_images / 127.5 - 1.0  # Normalize to the range [-1, 1]\n",
    "train_images = train_images.reshape(train_images.shape[0], 784)\n",
    "\n",
    "# Define the generator\n",
    "def build_generator(latent_dim):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(256, input_dim=latent_dim))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    generator.add(Reshape((28, 28, 1)))\n",
    "    return generator\n",
    "\n",
    "# Define the discriminator\n",
    "def build_discriminator(img_shape):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Flatten(input_shape=img_shape))\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    return discriminator\n",
    "\n",
    "# Build and compile the discriminator\n",
    "img_shape = (28, 28, 1)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build the GAN by connecting the generator and discriminator\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator.trainable = False\n",
    "gan = Sequential([generator, discriminator])\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "sample_interval = 1000\n",
    "\n",
    "# Training the GAN\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    idx = np.random.randint(0, train_images.shape[0], batch_size)\n",
    "    real_images = train_images[idx]\n",
    "    labels_real = np.ones((batch_size, 1))\n",
    "    labels_fake = np.zeros((batch_size, 1))\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    fake_images = generator.predict(noise)\n",
    "    d_loss_real = discriminator.train_on_batch(real_images, labels_real)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_images, labels_fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    labels_g = np.ones((batch_size, 1))\n",
    "    g_loss = gan.train_on_batch(noise, labels_g)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss[0]}, D Accuracy: {100*d_loss[1]}\", end=' ')\n",
    "    print(f\"G Loss: {g_loss[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca6a5a-e137-48d8-9054-5398423d08d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
